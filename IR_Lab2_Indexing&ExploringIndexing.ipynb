{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day2/IR_in_Arabic_Lab2_Indexing%26ExploringIndexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZzUxXUVMVVU"
   },
   "source": [
    "\n",
    "\n",
    "# **Information Retreival** - Winter 2024-2025 (SEM 1) lab notebook 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kjTczIiMctt"
   },
   "source": [
    "This is one of a series of Colab notebooks created for the **Information Retreival** course. It demonstrates how we can index a collection, and how to access an index to visualize some index analysis.\n",
    "\n",
    "The **learning outcomes** of the this notebook are:\n",
    "\n",
    "\n",
    "*   PyTerrier setup.\n",
    "*   Preprocessing.\n",
    "*   Indexing a collection.\n",
    "*   Accessing and exploring the index.\n",
    "\n",
    "What is PyTerrier?\n",
    "\n",
    "**[PyTerrier](https://pyterrier.readthedocs.io/en/latest/)** is a Python framework, but uses the underlying [Terrier information retrieval](http://terrier.org/) toolkit for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkS6LLkX6HHV"
   },
   "source": [
    "### **Setup**\n",
    "We will first install Pyterrier as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "A6sKgPMd_-gU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-terrier\n",
      "  Downloading python-terrier-0.11.0.tar.gz (119 kB)\n",
      "     ---------------------------------------- 0.0/119.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/119.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/119.5 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/119.5 kB 262.6 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/119.5 kB 262.6 kB/s eta 0:00:01\n",
      "     ------------------- ----------------- 61.4/119.5 kB 328.2 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 112.6/119.5 kB 504.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 119.5/119.5 kB 466.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (2.0.3)\n",
      "Collecting matchpy (from python-terrier)\n",
      "  Downloading matchpy-0.5.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: more_itertools in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (8.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (2.31.0)\n",
      "Collecting ir_datasets>=0.3.2 (from python-terrier)\n",
      "  Downloading ir_datasets-0.5.8-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: wget in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (3.2)\n",
      "Collecting pyjnius>=1.4.2 (from python-terrier)\n",
      "  Downloading pyjnius-1.6.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting deprecated (from python-terrier)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (1.11.1)\n",
      "Collecting ir_measures>=0.3.1 (from python-terrier)\n",
      "  Downloading ir_measures-0.3.3.tar.gz (48 kB)\n",
      "     ---------------------------------------- 0.0/48.8 kB ? eta -:--:--\n",
      "     ------------------------- -------------- 30.7/48.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 48.8/48.8 kB 621.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytrec_eval_terrier>=0.5.3 (from python-terrier)\n",
      "  Downloading pytrec_eval_terrier-0.5.6-cp311-cp311-win_amd64.whl.metadata (796 bytes)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: dill in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (0.3.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-terrier) (1.2.0)\n",
      "Collecting chest (from python-terrier)\n",
      "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from ir_datasets>=0.3.2->python-terrier) (4.12.2)\n",
      "Collecting inscriptis>=2.2.0 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading inscriptis-2.5.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from ir_datasets>=0.3.2->python-terrier) (4.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from ir_datasets>=0.3.2->python-terrier) (6.0)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Requirement already satisfied: lz4>=3.1.10 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from ir_datasets>=0.3.2->python-terrier) (4.3.2)\n",
      "Collecting warc3-wet>=0.2.3 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting zlib-state>=0.1.3 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading zlib_state-0.1.6-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting ijson>=3.1.3 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading ijson-3.3.0-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading unlzw3-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting cwl-eval>=1.0.10 (from ir_measures>=0.3.1->python-terrier)\n",
      "  Downloading cwl-eval-1.0.12.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests->python-terrier) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests->python-terrier) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests->python-terrier) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests->python-terrier) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tqdm->python-terrier) (0.4.6)\n",
      "Requirement already satisfied: heapdict in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from deprecated->python-terrier) (1.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from jinja2->python-terrier) (2.1.1)\n",
      "Collecting multiset<3.0,>=2.0 (from matchpy->python-terrier)\n",
      "  Downloading multiset-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pandas->python-terrier) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pandas->python-terrier) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pandas->python-terrier) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from statsmodels->python-terrier) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from statsmodels->python-terrier) (23.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir_datasets>=0.3.2->python-terrier) (2.4)\n",
      "Requirement already satisfied: six in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets>=0.3.2->python-terrier)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading ir_datasets-0.5.8-py3-none-any.whl (347 kB)\n",
      "   ---------------------------------------- 0.0/347.4 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 112.6/347.4 kB 6.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 286.7/347.4 kB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 337.9/347.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 347.4/347.4 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading pyjnius-1.6.1-cp311-cp311-win_amd64.whl (224 kB)\n",
      "   ---------------------------------------- 0.0/224.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 224.3/224.3 kB 13.4 MB/s eta 0:00:00\n",
      "Downloading pytrec_eval_terrier-0.5.6-cp311-cp311-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.9/56.9 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
      "   ---------------------------------------- 0.0/69.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 69.6/69.6 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading ijson-3.3.0-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/51.0 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/51.0 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/51.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.0/51.0 kB 373.7 kB/s eta 0:00:00\n",
      "Downloading inscriptis-2.5.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.4/45.4 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Downloading zlib_state-0.1.6-cp311-cp311-win_amd64.whl (12 kB)\n",
      "Building wheels for collected packages: python-terrier, ir_measures, chest, cwl-eval, warc3-wet-clueweb09, cbor\n",
      "  Building wheel for python-terrier (setup.py): started\n",
      "  Building wheel for python-terrier (setup.py): finished with status 'done'\n",
      "  Created wheel for python-terrier: filename=python_terrier-0.11.0-py3-none-any.whl size=129659 sha256=0b7291e6c1938c53908dfea0512ecea1d9beaa2e154b0f1f0b4b8cd959d49c9d\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\e5\\93\\74\\56475269f81b5a87ebc941c0b90ae0026f6115a230072f41f3\n",
      "  Building wheel for ir_measures (setup.py): started\n",
      "  Building wheel for ir_measures (setup.py): finished with status 'done'\n",
      "  Created wheel for ir_measures: filename=ir_measures-0.3.3-py3-none-any.whl size=61215 sha256=1fce641ed35ec2f1cce9cddb0e6130fafe541331eb76c95c6116f283c81d0979\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\2c\\8d\\c4\\5e0e9526226549c346d4f98de5654fd9975467543746e47651\n",
      "  Building wheel for chest (setup.py): started\n",
      "  Building wheel for chest (setup.py): finished with status 'done'\n",
      "  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7652 sha256=a095738f5bd1d97b56267534cdbe7c84a964712e65261c0b50226a88f15f898e\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\48\\57\\13\\28831e81239278141f874ee9b7d6d5490a1b1191c2d07a3e73\n",
      "  Building wheel for cwl-eval (setup.py): started\n",
      "  Building wheel for cwl-eval (setup.py): finished with status 'done'\n",
      "  Created wheel for cwl-eval: filename=cwl_eval-1.0.12-py3-none-any.whl size=38110 sha256=9a22b07f2b2217f96862511425742e739053eb18e61a9f98275e20fa05abb6c1\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\69\\b2\\12\\4e7d9dce1cd0d65b194a3284751974094d020da593d750e76a\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py): started\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py): finished with status 'done'\n",
      "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18940 sha256=2cbeba64d0c1bccd3868b348bcb4e0891c3d97110a071001e33c0ae09ec9b503\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\63\\f9\\dc\\2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
      "  Building wheel for cbor (setup.py): started\n",
      "  Building wheel for cbor (setup.py): finished with status 'done'\n",
      "  Created wheel for cbor: filename=cbor-1.0.0-py3-none-any.whl size=10040 sha256=a4bdd5209ab2dd0d2c1a4f2e69216f46ca309f7139ba618d274a6d22d020ad8a\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\21\\6b\\45\\0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
      "Successfully built python-terrier ir_measures chest cwl-eval warc3-wet-clueweb09 cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, pyjnius, multiset, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pytrec_eval_terrier, matchpy, deprecated, cwl-eval, chest, ir_measures, inscriptis, ir_datasets, python-terrier\n",
      "Successfully installed cbor-1.0.0 chest-0.2.3 cwl-eval-1.0.12 deprecated-1.2.14 ijson-3.3.0 inscriptis-2.5.0 ir_datasets-0.5.8 ir_measures-0.3.3 matchpy-0.5.5 multiset-2.1.1 pyjnius-1.6.1 python-terrier-0.11.0 pytrec_eval_terrier-0.5.6 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#install the Pyterrier framework\n",
    "!pip install python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIzygfXzAILT"
   },
   "source": [
    "The next step is to initialise PyTerrier. This is performed using PyTerrier's init() method. The init() method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent init() from being called more than once by checking started()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMSW5eI-TQGX"
   },
   "source": [
    "Another library that we need for this lab is Arabic-Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvXs6SpBZ6mD"
   },
   "source": [
    "We will import all the python libraries needed for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "L3cU_TmDZ_Jf"
   },
   "outputs": [],
   "source": [
    "#we need to import the following libraries.\n",
    "import pandas as pd\n",
    "#to display the full text on the notebook without truncation\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "import re\n",
    "from snowballstemmer import stemmer\n",
    "from sklearn.feature_extraction import _stop_words as stp\n",
    "#import arabicstopwords.arabicstopwords as stp\n",
    "#make your loops show a smart progress meter \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEFW83xhgeiy"
   },
   "source": [
    "### **What are DataFrames?** \n",
    "[Pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html): Two-dimensional, size-mutable, potentially heterogeneous tabular data. Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4q5o48EoXo_V"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>35</td>\n",
       "      <td>690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nada</td>\n",
       "      <td>45</td>\n",
       "      <td>460000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age  salary\n",
       "0   Ahmed   25   50000\n",
       "1  Fatima   35  690000\n",
       "2    Nada   45  460000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new dataframe\n",
    "my_df=pd.DataFrame([[\"Ahmed\",25,50000],[\"Fatima\",35,690000],[\"Nada\",45,460000]],columns=['name','age','salary'])\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vNHjYzKFiHXO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>35</td>\n",
       "      <td>690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nada</td>\n",
       "      <td>45</td>\n",
       "      <td>460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salwa</td>\n",
       "      <td>24</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age  salary\n",
       "0   Ahmed   25   50000\n",
       "1  Fatima   35  690000\n",
       "2    Nada   45  460000\n",
       "3   Salwa   24   90000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert a new row\n",
    "import pandas as pd\n",
    "\n",
    "# Your existing DataFrame (assuming my_df already exists)\n",
    "data = {'name': [\"Salwa\"], 'age': [24], 'salary': [90000]}\n",
    "new_row = pd.DataFrame(data)\n",
    "\n",
    "# Concatenate the new row to the original DataFrame\n",
    "my_df = pd.concat([my_df, new_row], ignore_index=True)\n",
    "\n",
    "# Huzzah! The DataFrame has grown!\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "MhBQoYJZlDM5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nada</td>\n",
       "      <td>460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salwa</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  salary\n",
       "0   Ahmed   50000\n",
       "1  Fatima  690000\n",
       "2    Nada  460000\n",
       "3   Salwa   90000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print just name and salary\n",
    "my_df[['name','salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "CL-u3Bu6ihG-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>35</td>\n",
       "      <td>690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nada</td>\n",
       "      <td>45</td>\n",
       "      <td>460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salwa</td>\n",
       "      <td>24</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age  salary\n",
       "1  Fatima   35  690000\n",
       "2    Nada   45  460000\n",
       "3   Salwa   24   90000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the data about people with salary>60000\n",
    "my_df[my_df['salary']>60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "TxIxTwp5itXW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed</td>\n",
       "      <td>25</td>\n",
       "      <td>51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>35</td>\n",
       "      <td>691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nada</td>\n",
       "      <td>45</td>\n",
       "      <td>461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salwa</td>\n",
       "      <td>24</td>\n",
       "      <td>91000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age  salary\n",
       "0   Ahmed   25   51000\n",
       "1  Fatima   35  691000\n",
       "2    Nada   45  461000\n",
       "3   Salwa   24   91000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increase the salary of all by 1000\n",
    "def increase_salary(salary):\n",
    "    return salary+1000\n",
    "    \n",
    "my_df[\"salary\"]=my_df[\"salary\"].apply(increase_salary)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwvYVmK5WOD-"
   },
   "source": [
    "### **Data preparation**\n",
    "We will first create five textual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "AoD-lm5-YSLx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0</td>\n",
       "      <td>This is the first day of the information retrieval course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1</td>\n",
       "      <td>The course is in Arabic for Arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2</td>\n",
       "      <td>Today is May 30, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3</td>\n",
       "      <td>We hope this course will benefit Arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4</td>\n",
       "      <td>Are you happy with this experience?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docno                                                   raw_text\n",
       "0    d0  This is the first day of the information retrieval course\n",
       "1    d1                  The course is in Arabic for Arab students\n",
       "2    d2                                      Today is May 30, 2021\n",
       "3    d3             We hope this course will benefit Arab students\n",
       "4    d4                        Are you happy with this experience?"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df = pd.DataFrame([ [\"d0\", \"This is the first day of the information retrieval course\"],\n",
    "[\"d1\", \"The course is in Arabic for Arab students\"],\n",
    "[\"d2\", \"Today is May 30, 2021\"],\n",
    "[\"d3\", \"We hope this course will benefit Arab students\"],\n",
    "[\"d4\", \"Are you happy with this experience?\"]],\n",
    "                        columns=[\"docno\", \"raw_text\"])\n",
    "\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyKXDJf0p_Lp"
   },
   "source": [
    "Before indexing our data we need to do the following processing steps:\n",
    "\n",
    "\n",
    "1.   **Remove stopwords.**\n",
    "2.   **Normalization.**\n",
    "3.   **Stemming.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB-qA3T9qoBs"
   },
   "source": [
    "\n",
    "Let's remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "A59dw1U0Z_QU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stp.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "54iZRAN2lbhX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stp.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ceLmvDiTq6oW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************documents after removing stopwords*********************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0</td>\n",
       "      <td>This is the first day of the information retrieval course</td>\n",
       "      <td>This day information retrieval course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1</td>\n",
       "      <td>The course is in Arabic for Arab students</td>\n",
       "      <td>The course Arabic Arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2</td>\n",
       "      <td>Today is May 30, 2021</td>\n",
       "      <td>Today May 30, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3</td>\n",
       "      <td>We hope this course will benefit Arab students</td>\n",
       "      <td>We hope course benefit Arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4</td>\n",
       "      <td>Are you happy with this experience?</td>\n",
       "      <td>Are happy experience?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docno                                                   raw_text  \\\n",
       "0    d0  This is the first day of the information retrieval course   \n",
       "1    d1                  The course is in Arabic for Arab students   \n",
       "2    d2                                      Today is May 30, 2021   \n",
       "3    d3             We hope this course will benefit Arab students   \n",
       "4    d4                        Are you happy with this experience?   \n",
       "\n",
       "                                    text  \n",
       "0  This day information retrieval course  \n",
       "1        The course Arabic Arab students  \n",
       "2                     Today May 30, 2021  \n",
       "3   We hope course benefit Arab students  \n",
       "4                  Are happy experience?  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing Stop Words function\n",
    "def remove_stopWords(sentence):\n",
    "    terms=[]\n",
    "    stopWords= set(stp.ENGLISH_STOP_WORDS)\n",
    "    for term in sentence.split() : \n",
    "        if term not in stopWords :\n",
    "           terms.append(term)\n",
    "    return \" \".join(terms)\n",
    "\n",
    "docs_df[\"text\"]=docs_df[\"raw_text\"].apply(remove_stopWords)\n",
    "print(\"***************************************************************************documents after removing stopwords*********************************************************************\")\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_dkV6D4rgOI"
   },
   "source": [
    "After removing the stopwords the next step is to normalize our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bnQz-uyzrsRd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this day information retrieval course\n",
      "this day information retrieval course\n",
      "this day information retrieval course\n",
      "the course arabic arab students\n",
      "the course arabic arab students\n",
      "the course arabic arab students\n",
      "today may 30, 2021\n",
      "today may  \n",
      "today may\n",
      "we hope course benefit arab students\n",
      "we hope course benefit arab students\n",
      "we hope course benefit arab students\n",
      "are happy experience?\n",
      "are happy experience\n",
      "are happy experience\n",
      "***************************************************************************documents after normalizing*********************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0</td>\n",
       "      <td>This is the first day of the information retrieval course</td>\n",
       "      <td>this day information retrieval course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1</td>\n",
       "      <td>The course is in Arabic for Arab students</td>\n",
       "      <td>the course arabic arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2</td>\n",
       "      <td>Today is May 30, 2021</td>\n",
       "      <td>today may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3</td>\n",
       "      <td>We hope this course will benefit Arab students</td>\n",
       "      <td>we hope course benefit arab students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4</td>\n",
       "      <td>Are you happy with this experience?</td>\n",
       "      <td>are happy experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docno                                                   raw_text  \\\n",
       "0    d0  This is the first day of the information retrieval course   \n",
       "1    d1                  The course is in Arabic for Arab students   \n",
       "2    d2                                      Today is May 30, 2021   \n",
       "3    d3             We hope this course will benefit Arab students   \n",
       "4    d4                        Are you happy with this experience?   \n",
       "\n",
       "                                    text  \n",
       "0  this day information retrieval course  \n",
       "1        the course arabic arab students  \n",
       "2                              today may  \n",
       "3   we hope course benefit arab students  \n",
       "4                   are happy experience  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a function to normalize the tweets\n",
    "\n",
    "      \n",
    "def normalize(text):\n",
    "    lower_string = text.lower()\n",
    "    print(lower_string)\n",
    "    # Remove punctuation and numbers\n",
    "    cleaned_string = re.sub(r'[^a-zA-Z\\s]', '', lower_string)\n",
    "    print(cleaned_string)\n",
    "    normalized_string = ' '.join(cleaned_string.split())\n",
    "    print(normalized_string)\n",
    "    return(normalized_string)\n",
    "\n",
    "docs_df[\"text\"]=docs_df[\"text\"].apply(normalize)\n",
    "print(\"***************************************************************************documents after normalizing*********************************************************************\")\n",
    "docs_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZY1yWSgr_jj"
   },
   "source": [
    "The last processing step is to stem the terms in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mJbx-DfWnEIw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************documents after stemming*********************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0</td>\n",
       "      <td>This is the first day of the information retrieval course</td>\n",
       "      <td>this day inform retriev cours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1</td>\n",
       "      <td>The course is in Arabic for Arab students</td>\n",
       "      <td>the cours arab arab student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2</td>\n",
       "      <td>Today is May 30, 2021</td>\n",
       "      <td>today may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3</td>\n",
       "      <td>We hope this course will benefit Arab students</td>\n",
       "      <td>we hope cours benefit arab student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4</td>\n",
       "      <td>Are you happy with this experience?</td>\n",
       "      <td>are happi experi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docno                                                   raw_text  \\\n",
       "0    d0  This is the first day of the information retrieval course   \n",
       "1    d1                  The course is in Arabic for Arab students   \n",
       "2    d2                                      Today is May 30, 2021   \n",
       "3    d3             We hope this course will benefit Arab students   \n",
       "4    d4                        Are you happy with this experience?   \n",
       "\n",
       "                                 text  \n",
       "0       this day inform retriev cours  \n",
       "1         the cours arab arab student  \n",
       "2                           today may  \n",
       "3  we hope cours benefit arab student  \n",
       "4                    are happi experi  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify that we want to stem arabic text\n",
    "stemmerObj = stemmer(\"english\")  # Use \"english\" or another supported language\n",
    "#define the stemming function\n",
    "def stem(sentence):\n",
    "    return \" \".join([stemmerObj.stemWord(i) for i in sentence.split()])\n",
    "\n",
    "docs_df['text']=docs_df['text'].apply(stem)\n",
    "print(\"***************************************************************************documents after stemming*********************************************************************\")\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA-HvUt_Yedy"
   },
   "source": [
    "Next, we will index the dataframe's documents. The index, with all its data structures, is saved into a directory called **myFirstIndex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "OOwBC5Rwt8jk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\ipykernel_1184\\3929588820.py:1: DeprecationWarning: Call to deprecated class DFIndexer. (use pt.terrier.IterDictIndexer().index(dataframe.to_dict(orient='records')) instead) -- Deprecated since version 0.11.0.\n",
      "  indexer = pt.DFIndexer(\"./myFirstIndex\", overwrite=True)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find JAVA_HOME",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m indexer \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mDFIndexer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./myFirstIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#as the default is an English tokenizer we will update it by setting it to a non-English tokenizer \"UTFTokenizer\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m indexer\u001b[38;5;241m.\u001b[39msetProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokeniser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTFTokeniser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyterrier\\utils.py:206\u001b[0m, in \u001b[0;36mpre_invocation_decorator.<locals>._decorator_wrapper.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 206\u001b[0m     decorator(fn)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyterrier\\java\\_utils.py:32\u001b[0m, in \u001b[0;36mrequired\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _started:\n\u001b[0;32m     31\u001b[0m     trigger \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 32\u001b[0m     _init(trigger\u001b[38;5;241m=\u001b[39mtrigger)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyterrier\\utils.py:94\u001b[0m, in \u001b[0;36monce.<locals>._once.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has already been run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# how to handle errors?\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m res \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyterrier\\java\\_utils.py:134\u001b[0m, in \u001b[0;36m_init\u001b[1;34m(trigger)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, initializer \u001b[38;5;129;01min\u001b[39;00m initializers:\n\u001b[0;32m    132\u001b[0m     initializer\u001b[38;5;241m.\u001b[39mpre_init(jnius_config)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjnius\u001b[39;00m \u001b[38;5;66;03m# noqa: PT100 \u001b[39;00m\n\u001b[0;32m    135\u001b[0m _started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# run post-initialization setup\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jnius\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m     17\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     java \u001b[38;5;241m=\u001b[39m get_java_setup(sys\u001b[38;5;241m.\u001b[39mplatform)\n\u001b[0;32m     19\u001b[0m     jdk_home \u001b[38;5;241m=\u001b[39m java\u001b[38;5;241m.\u001b[39mget_javahome()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39madd_dll_directory(path):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jnius\\env.py:60\u001b[0m, in \u001b[0;36mget_java_setup\u001b[1;34m(platform)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# go hunting for Javac and Java programs, in that order\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[1;32m---> 60\u001b[0m     JAVA_HOME \u001b[38;5;241m=\u001b[39m get_jdk_home(platform)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[0;32m     63\u001b[0m     JAVA_HOME \u001b[38;5;241m=\u001b[39m get_jre_home(platform)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jnius\\env.py:335\u001b[0m, in \u001b[0;36mget_jdk_home\u001b[1;34m(platform)\u001b[0m\n\u001b[0;32m    333\u001b[0m TMP_JDK_HOME \u001b[38;5;241m=\u001b[39m getenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJAVA_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TMP_JDK_HOME:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to find JAVA_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# Remove /bin if it's appended to JAVA_HOME\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TMP_JDK_HOME[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mException\u001b[0m: Unable to find JAVA_HOME"
     ]
    }
   ],
   "source": [
    "indexer = pt.DFIndexer(\"./myFirstIndex\", overwrite=True)\n",
    "#as the default is an English tokenizer we will update it by setting it to a non-English tokenizer \"UTFTokenizer\"\n",
    "indexer.setProperty(\"tokeniser\", \"UTFTokeniser\")\n",
    "# index the text, record the docnos as metadata\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "index_ref.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing 'python': ['frequency_bigramdictionary_en_243_342.txt', 'frequency_bigramdictionary_en_243_342.txt', 'frequency_bigramdictionary_en_243_342.txt', 'frequency_bigramdictionary_en_243_342.txt', 'frequency_bigramdictionary_en_243_342.txt', 'frequency_dictionary_en_82_765.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_inverted_index(directory_path):\n",
    "    inverted_index = {}  # Initialize an empty dictionary\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read().lower()  # Read the file content and convert to lowercase\n",
    "                terms = content.split()  # Split content into terms (words)\n",
    "\n",
    "                for term in terms:\n",
    "                    if term not in inverted_index:\n",
    "                        inverted_index[term] = [filename]\n",
    "                    else:\n",
    "                        inverted_index[term].append(filename)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Example usage:\n",
    "text_files_directory = '.'\n",
    "inverted_index = create_inverted_index(text_files_directory)\n",
    "\n",
    "# Now you can look up terms and find the corresponding documents\n",
    "search_term = 'python'\n",
    "if search_term in inverted_index:\n",
    "    print(f\"Documents containing '{search_term}': {inverted_index[search_term]}\")\n",
    "else:\n",
    "    print(f\"'{search_term}' not found in any documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FVdZjX5vHMg"
   },
   "source": [
    "### **Exercise1**\n",
    "How many documents mention your country name? which documents are those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fT2ORK4HwLBj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DElk2mxEvOoE"
   },
   "source": [
    "### **Exercise2**\n",
    "Select any document from the collection and check which of its terms appear in the index?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lwNWMCNwKaM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAhILb3S_ChS"
   },
   "source": [
    "### **Exercise3**\n",
    "How can we update our index to include the positions of the terms in the index? Hint: you can use [PyTerrier documentation](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baW1NO1l--hC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8ZUAJKhwL0J"
   },
   "source": [
    "### **Exercise4**\n",
    "Index an Arabic collection of your choice. You can use the Arabic datasets available at [Huggingface](https://huggingface.co/datasets?filter=languages:ar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrftyM9kwFc9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71_fMCB2FXIG"
   },
   "source": [
    "### **References**\n",
    "\n",
    "\n",
    "* [Pandas DataFrames documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).  \n",
    "* IR From Bag-of-words to BERT and Beyond through Practical Experiments. [PyTerrier ECIR2021 Tutorial](https://github.com/terrier-org/ecir2021tutorial).\n",
    "*   [PyTerrier documentation.](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/)\n",
    "* [Processing Arabic text in Python](https://alraqmiyyat.github.io/2013/01-02.html).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IR-in-Arabic_Lab2-Indexing&ExploringIndexing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
